{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOEse17ISleEWVS2BJEpPWp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saragamilmohamed/NLP/blob/main/Amazon%20Reviews/Polarity%20and%20%20cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjnd5axyfgI5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/amazon_alexa.tsv', sep='\\t')"
      ],
      "metadata": {
        "id": "lOgL-qshfqna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "d4yvnTsUf7kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "caKmQ-oUgQ5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "I6RZDK7Of-Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "KGbh2c0OgCYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "OeEangW5gFvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "gJKEJ8HigJFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['length'] = df['verified_reviews'].astype(str).apply(len)"
      ],
      "metadata": {
        "id": "QefBfv20hy1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_polarity(text):\n",
        "    textblob= TextBlob(str(text.encode('utf_8')))\n",
        "    pol = textblob.sentiment.polarity\n",
        "    return pol\n",
        "df['polarity']=df['verified_reviews'].astype(str).apply(get_polarity)"
      ],
      "metadata": {
        "id": "C_BVxb4fiADv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "eQiJcqtijH84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subjectivity(text):\n",
        "    textblob= TextBlob(str(text.encode('utf_8')))\n",
        "    pol = textblob.sentiment.subjectivity\n",
        "    return pol\n",
        "df['subjectivity']=df['verified_reviews'].astype(str).apply(get_subjectivity)"
      ],
      "metadata": {
        "id": "Qx5ewQ0NjKYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Pkw-pV62pJEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['char_count']=df['verified_reviews'].astype(str).apply(len)"
      ],
      "metadata": {
        "id": "krRU3ZSapOH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_count']=df['verified_reviews'].astype(str).apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "gmQMkDRHrfln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_density']=df['char_count']/(df['word_count']+1)"
      ],
      "metadata": {
        "id": "uGLezP5Drs2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "5ASJKOdwsJGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "punctuation=string.punctuation"
      ],
      "metadata": {
        "id": "G17DywjwsKgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['punctuation_count']=df['verified_reviews'].astype(str).apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))"
      ],
      "metadata": {
        "id": "lRtXYuA6siZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "nZHHhT4TtKPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['char_count','word_count','word_density','punctuation_count']].describe()"
      ],
      "metadata": {
        "id": "CXZuoXaItMLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "5BDadnLrv8n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "id": "MyzukbTOy_Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_dic = {\n",
        "    'noun': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
        "    'pron': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
        "    'verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
        "    'adj': ['JJ', 'JJR', 'JJS'],\n",
        "    'adv': ['RB', 'RBR', 'RBS', 'WRB']\n",
        "}\n",
        "\n",
        "def pos_check(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = tup[1]\n",
        "            if ppo in pos_dic[flag]:\n",
        "                cnt += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {x}\")\n",
        "        print(f\"Exception: {e}\")\n",
        "    return cnt\n",
        "\n",
        "df['noun_count'] = df['verified_reviews'].astype(str).apply(lambda x: pos_check(x, 'noun'))\n",
        "print(df)"
      ],
      "metadata": {
        "id": "3tXigYXmtwnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nlmko9VcwwHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['verified_reviews'][3]"
      ],
      "metadata": {
        "id": "nkslwtEkzgTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pron_count'] = df['verified_reviews'].astype(str).apply(lambda x: pos_check(x, 'pron'))"
      ],
      "metadata": {
        "id": "G1rJon6Uz04D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['verb_count'] = df['verified_reviews'].astype(str).apply(lambda x: pos_check(x, 'verb'))"
      ],
      "metadata": {
        "id": "4k5PBJNp1sXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['adj_count'] = df['verified_reviews'].astype(str).apply(lambda x: pos_check(x, 'adj'))"
      ],
      "metadata": {
        "id": "Ee_57lBU1xvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['adv_count'] = df['verified_reviews'].astype(str).apply(lambda x: pos_check(x, 'adv'))"
      ],
      "metadata": {
        "id": "fWvX2pvo14p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "JASKoJULwzPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def punctuation_removal(messy_str) :\n",
        "  if isinstance(messy_str, str) :\n",
        "      clean_list = [char for char in messy_str if char not in string.punctuation]\n",
        "      clean_str = ''.join(clean_list)\n",
        "      return clean_str\n",
        "\n",
        "  else:\n",
        "      return messy_str\n",
        "\n",
        "\n",
        "df['verified_reviews'] = df['verified_reviews'].apply(punctuation_removal)"
      ],
      "metadata": {
        "id": "nkpSmBsI2FTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets make a function to remove Numbers from the reviews\n",
        "import re\n",
        "def drop_numbers(list_text):\n",
        "    list_text_new = []\n",
        "    for i in list_text:\n",
        "        if not re.search('\\d', i):\n",
        "            list_text_new.append(i)\n",
        "    return ''.join(list_text_new)\n",
        "\n",
        "df['verified_reviews'] = df['verified_reviews'].astype(str).apply(drop_numbers)"
      ],
      "metadata": {
        "id": "ihILXTCnBQLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets visualize the Top 10 Reviews after Removal of Punctuations and Numbers\n",
        "df['verified_reviews'].head(10)"
      ],
      "metadata": {
        "id": "8f8uaygjBFAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets create a function to remove accented characters\n",
        "import unicodedata # Import the unicodedata module        é\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return new_text\n",
        "\n",
        "# lets apply the function\n",
        "df['verified_reviews'] = df.apply(lambda x: remove_accented_chars(x['verified_reviews']), axis = 1)"
      ],
      "metadata": {
        "id": "RHBbf4x9Fauy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a function to remove special characters\n",
        "def remove_special_characters(text):\n",
        "    pat = r'[^a-zA-z0-9]'\n",
        "    return re.sub(pat, ' ', text)\n",
        "\n",
        "# lets apply this function\n",
        "df['verified_reviews'] = df.apply(lambda x: remove_special_characters(x['verified_reviews']), axis = 1)"
      ],
      "metadata": {
        "id": "8tmNGZhdBj43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['verified_reviews'][:5]"
      ],
      "metadata": {
        "id": "r6oou_neFv-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = (10, 4)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['polarity'])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['subjectivity'])\n",
        "\n",
        "plt.suptitle('Distribution of Polarity and Subjectivity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PsnNm12tGxIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = (10, 4)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.distplot(df['polarity'])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.distplot(df['subjectivity'])\n",
        "\n",
        "plt.suptitle('Distribution of Polarity and Subjectivity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ntQmfKiEfbmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check relation between Polarity and Subjectivity\n",
        "\n",
        "sns.scatterplot(x=df['polarity'], y=df['subjectivity'])\n",
        "plt.title('Polarity vs Subjectivity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xRoIt_2IXNCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "cv = CountVectorizer(stop_words = 'english')\n",
        "words = cv.fit_transform(df.verified_reviews)\n",
        "sum_words = words.sum(axis=0)\n",
        "\n",
        "\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "color = plt.cm.ocean(np.linspace(0, 1, 20))\n",
        "frequency.tail(20).plot(x='word', y='freq', kind='bar', figsize=(15, 6), color = color)\n",
        "plt.title(\"Least Frequently Occuring Words - Top 20\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7-g6AvYTYM-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words.sum(axis=0)"
      ],
      "metadata": {
        "id": "N-a6xniMYvLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "cv = CountVectorizer(stop_words = 'english')\n",
        "words = cv.fit_transform(df.verified_reviews)\n",
        "sum_words = words.sum(axis=0)\n",
        "\n",
        "\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "wordcloud = WordCloud(background_color = 'lightcyan', width = 2000, height = 2000).generate_from_frequencies(dict(words_freq))\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.axis('off')\n",
        "plt.imshow(wordcloud)\n",
        "plt.title(\"Vocabulary from Reviews\", fontsize = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "poTBMmorZuhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]"
      ],
      "metadata": {
        "id": "WYzjX5GldkiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)"
      ],
      "metadata": {
        "id": "xjZPnJqNeCT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_freq[1]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jwLtZSI8dOZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# function for making ngrams\n",
        "from nltk.util import ngrams"
      ],
      "metadata": {
        "id": "0R5HHgwAZ30B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = str(df['verified_reviews'])\n",
        "tokenized = text.split()\n",
        "\n",
        "# and get a list of all the bi-grams\n",
        "esBigrams = ngrams(tokenized, 2)\n",
        "\n",
        "# get the frequency of each bigram in our corpus\n",
        "esBigramFreq = collections.Counter(esBigrams)\n",
        "\n",
        "# what are the ten most popular ngrams in this Spanish corpus?\n",
        "esBigramFreq.most_common(10)"
      ],
      "metadata": {
        "id": "9CxAMQqNcUqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the Trigrams\n",
        "\n",
        "text = str(df['verified_reviews'])\n",
        "tokenized = text.split()\n",
        "\n",
        "# and get a list of all the bi-grams\n",
        "esTrigrams = ngrams(tokenized, 3)\n",
        "\n",
        "# get the frequency of each bigram in our corpus\n",
        "esTrigramFreq = collections.Counter(esTrigrams)\n",
        "\n",
        "# what are the ten most popular ngrams in this Spanish corpus?\n",
        "esTrigramFreq.most_common(10)"
      ],
      "metadata": {
        "id": "SZFcesHrcWUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}